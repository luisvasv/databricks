-- Databricks notebook source
-- MAGIC %md
-- MAGIC # DELTA TABLES

-- COMMAND ----------

USE test;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## GET TABLE INFO

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### DESCRIBE TABLE

-- COMMAND ----------

DESCRIBE test.dml;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### DESCRIBE EXTENDED
-- MAGIC 
-- MAGIC `DESCRIBE [TABLE] EXTENDED schema.table`

-- COMMAND ----------

DESCRIBE EXTENDED test.dml;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### SHOW EXTENDED
-- MAGIC 
-- MAGIC `SHOW TABLE EXTENDED [ { IN | FROM } schema_name ] LIKE regex_pattern [ PARTITION clause ]`

-- COMMAND ----------

SHOW TABLE EXTENDED in test LIKE 'dml';

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### SHOW CREATE

-- COMMAND ----------

SHOW CREATE TABLE test.dml;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## DESCRIBE HISTORY
-- MAGIC 
-- MAGIC Returns provenance information, including the operation, user, and so on, for each write to a table. Table history is retained for 30 days.
-- MAGIC 
-- MAGIC **COLUMNS**
-- MAGIC 
-- MAGIC | Column              | Type      | Description                                                                |
-- MAGIC |---------------------|-----------|----------------------------------------------------------------------------|
-- MAGIC | version             | long      | Table version generated by the operation.                                  |
-- MAGIC | timestamp           | timestamp | When this version was committed.                                           |
-- MAGIC | userId              | string    | ID of the user that ran the operation.                                     |
-- MAGIC | userName            | string    | Name of the user that ran the operation.                                   |
-- MAGIC | operation           | string    | Name of the operation.                                                     |
-- MAGIC | operationParameters | map       | Parameters of the operation (for example, predicates.)                     |
-- MAGIC | job                 | struct    | Details of the job that ran the operation.                                 |
-- MAGIC | notebook            | struct    | Details of notebook from which the operation was run.                      |
-- MAGIC | clusterId           | string    | ID of the cluster on which the operation ran.                              |
-- MAGIC | readVersion         | long      | Version of the table that was read to perform the write operation.         |
-- MAGIC | isolationLevel      | string    | Isolation level used for this operation.                                   |
-- MAGIC | isBlindAppend       | boolean   | Whether this operation appended data.                                      |
-- MAGIC | operationMetrics    | map       | Metrics of the operation (for example, number of rows and files modified.) |
-- MAGIC | userMetadata        | string    | User-defined commit metadata if it was specified                           |

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### TABLE

-- COMMAND ----------

DESCRIBE HISTORY dml;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### PATH

-- COMMAND ----------

DESCRIBE HISTORY '/mnt/bronze/table_dml';

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### DELTA PATH

-- COMMAND ----------

DESCRIBE HISTORY delta.`/mnt/bronze/table_dml`

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### LAST OPERATION

-- COMMAND ----------

DESCRIBE HISTORY dml LIMIT 1  

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### READ DELTA LOG

-- COMMAND ----------

-- MAGIC %python
-- MAGIC 
-- MAGIC df = spark.read.format("json").load("/mnt/bronze/table_dml/_delta_log/*.json")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC display(df)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## DESCRIBE DETAIL
-- MAGIC 
-- MAGIC it retrieve detailed information about a Delta table (for example, number of files, data size) 

-- COMMAND ----------

DESCRIBE DETAIL test.dml;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## TIME TRAVEL
-- MAGIC 
-- MAGIC We can use a query an old snapshot of a table using time travel. Time travel is a specialized read on our dataset which allows us to read previous versions of our data. There are several ways to go about this. 
-- MAGIC 
-- MAGIC The `@` symbol can be used with a version number, aliased to `v#`, like the syntax below.
-- MAGIC 
-- MAGIC The `@` symbol can also be used with a version number or a datestamp in the format: `yyyyMMddHHmmssSSS`

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### @V#

-- COMMAND ----------

SELECT * FROM dml@v1

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### VERSION AS OF #

-- COMMAND ----------

SELECT * FROM dml VERSION AS OF 2

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## OPTIMIZE
-- MAGIC 
-- MAGIC Optimizes the layout of Delta Lake data. Optionally optimize a subset of data or colocate data by column. If you do not specify colocation, bin-packing optimization is performed.
-- MAGIC 
-- MAGIC ```
-- MAGIC OPTIMIZE table_name [WHERE predicate]
-- MAGIC     [ZORDER BY (col_name1 [, ...] ) ]
-- MAGIC ```
-- MAGIC 
-- MAGIC **OPTIMIZATION ALGORITHMS**
-- MAGIC 
-- MAGIC 
-- MAGIC 
-- MAGIC * **bin-packing:** improve this speed is to coalesce small files into larger ones.
-- MAGIC 
-- MAGIC * **z-Ordering:** Z-Ordering is a technique to colocate related information in the same set of files. This co-locality is automatically used by Delta Lake on Databricks data-skipping algorithms. This behavior dramatically reduces the amount of data that Delta Lake on Databricks needs to read.
-- MAGIC 
-- MAGIC 
-- MAGIC 
-- MAGIC **ANOTATIONS**
-- MAGIC 
-- MAGIC * **WHERE:** optimize the subset of rows matching the given partition predicate. Only filters involving partition key attributes are supported.
-- MAGIC 
-- MAGIC * **ZORDER BY:** colocate column information in the same set of files. Co-locality is used by Delta Lake data-skipping algorithms to dramatically reduce the amount of data that needs to be read. You can specify multiple columns for ZORDER BY as a comma-separated list. However, the effectiveness of the locality drops with each additional column.

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### GENERAL

-- COMMAND ----------

SELECT *, input_file_name() FROM dml

-- COMMAND ----------

OPTIMIZE dml;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### WHERE

-- COMMAND ----------

OPTIMIZE dml WHERE active = 3;

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### ZORDER

-- COMMAND ----------

OPTIMIZE dml 
ZORDER BY (citydc);

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## ** BLOOM FILTER INDEX
-- MAGIC 
-- MAGIC 
-- MAGIC ## Bloom Filter Indexes
-- MAGIC 
-- MAGIC While Z-order provides useful data clustering for high cardinality data, it's often most effective when working with queries that filter against continuous numeric variables.
-- MAGIC 
-- MAGIC Bloom filters provide an efficient algorithm for probabilistically identifying files that may contain data using fields containing arbitrary text. Appropriate fields would include hashed values, alphanumeric codes, or free-form text fields.
-- MAGIC 
-- MAGIC Bloom filters calculate indexes that indicate the likelihood a given value **could** be in a file; the size of the calculated index will vary based on the number of unique values present in the field being indexed and the configured tolerance for false positives.
-- MAGIC 
-- MAGIC **NOTE**: A false positive would be a file that the index thinks could have a matching record but does not. Files containing data matching a selective filter will never be skipped; false positives just mean that extra time was spent scanning files without matching records.
-- MAGIC 
-- MAGIC Looking at the distribution for the `key` field, this is an ideal candidate for this technique.
-- MAGIC 
-- MAGIC ---
-- MAGIC 
-- MAGIC 
-- MAGIC is a space-efficient data structure that enables data skipping on chosen columns, particularly for fields containing arbitrary text.
-- MAGIC 
-- MAGIC 
-- MAGIC https://docs.databricks.com/delta/optimizations/bloom-filters.html
-- MAGIC 
-- MAGIC 
-- MAGIC 
-- MAGIC 
-- MAGIC %sql
-- MAGIC CREATE BLOOMFILTER INDEX
-- MAGIC ON TABLE date_part_table
-- MAGIC FOR COLUMNS(key OPTIONS (fpp=0.1, numItems=200))

-- COMMAND ----------



-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## AUTO OPTIMIZE
-- MAGIC 
-- MAGIC https://docs.databricks.com/delta/optimizations/auto-optimize.html

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##  VACUUM
-- MAGIC 
-- MAGIC cleans up files associated with a table. There are different versions of this command for Delta and Apache Spark tables.
-- MAGIC 
-- MAGIC **IMPORTANT:** If you run `VACUUM` on a Delta table, you lose the ability to time travel back to a version older than the specified data retention period.
-- MAGIC 
-- MAGIC `VACUUM table_name [RETAIN num HOURS] [DRY RUN]`
-- MAGIC 
-- MAGIC 
-- MAGIC 
-- MAGIC 
-- MAGIC * **RETAIN num HOURS** : the retention threshold.
-- MAGIC 
-- MAGIC * **DRY RUN**: return a list of files to be deleted
-- MAGIC 
-- MAGIC 
-- MAGIC **DATABRICKS RECOMENDATION**
-- MAGIC 
-- MAGIC It is recommended that you set a retention interval to be at least 7 days, because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table. If `VACUUM` cleans up active files, concurrent readers can fail or, worse, tables can be corrupted when `VACUUM` deletes files that have not yet been committed. You must choose an interval that is longer than the longest running concurrent transaction and the longest period that any stream can lag behind the most recent update to the table.

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##  RESTORE
-- MAGIC 
-- MAGIC You can restore a Delta table to its earlier state by using the `RESTORE` command. A Delta table internally maintains historic versions of the table that enable it to be restored to an earlier state

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ###  TABLE
-- MAGIC 
-- MAGIC `RESTORE TABLE db.target_table TO VERSION AS OF <version>`

-- COMMAND ----------

-- TODO

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ###  LOCATION
-- MAGIC 
-- MAGIC `RESTORE TABLE delta.`/data/target/` TO TIMESTAMP AS OF <timestamp>`

-- COMMAND ----------

-- TODO

-- COMMAND ----------

VACUUM dml RETAIN 168 HOURS DRY RUN

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ###  VACUUM FOR CLONED TABLES

-- COMMAND ----------

spark.conf.set("spark.databricks.delta.retentionDurationCheck.enabled", False)
spark.sql("VACUUM sensors_prod RETAIN 0 HOURS")
spark.conf.set("spark.databricks.delta.retentionDurationCheck.enabled", True)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ###  VACUUM FOR TBLPROPERTIES

-- COMMAND ----------

ALTER TABLE sensors_backup
SET TBLPROPERTIES (
  delta.logRetentionDuration = '3650 days',
  delta.deletedFileRetentionDuration = '3650 days'
)

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ##  GENERATE MANIFEST
-- MAGIC 
-- MAGIC manifest file for a Delta table that can be used by other processing engines (that is, other than Apache `Spark`) to read the Delta table. For example, to generate a manifest file that can be used by Presto and Athena to read a Delta table
-- MAGIC 
-- MAGIC Note: for the moment is only supported on AWS S3 for other cloud providers you will see:
-- MAGIC 
-- MAGIC ```
-- MAGIC 
-- MAGIC Error in SQL statement: UnsupportedOperationException: 
-- MAGIC Generation of manifests for Delta table is currently only supported on AWS S3.
-- MAGIC To disable this check, run the following SQL command.
-- MAGIC    SET spark.databricks.delta.symlinkFormatManifest.fileSystemCheck.enabled = false
-- MAGIC ```

-- COMMAND ----------

GENERATE symlink_format_manifest FOR TABLE delta.`/mnt/bronze/table_dml`
